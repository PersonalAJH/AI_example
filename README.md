# AI_example
Various applications using AI


에이전트 입장에서 루프
1. 현재상황 st 에서 어떤 액션을 해야할지 at를 결정
2. 결정된 행동 at 를 환경으로 보냄
3. 환경으로부터 그에 따른 보상과 다음상태의 정보를 받음

Markov Process
-미리 정의된 어떠한 확률분포를 따라 상태와 상태를 전이해 가는 과정
-상태집합 S(에이전트가 될 수 있는 상황들) 와 전이 확률 행렬 P(상태에서 다른상태로 이동하는 확률분포표, 하나의 상태에서 뻗어나가는 모든 확률의 합은 1이다)가 필요하다.
-Markov Property : 이전의 상태는 현재의 상태의 확률분포에 영향을 줄 수 없으며 오직 현재상태에 의해서만 전이확률분포가 결정된다.

마르코프한 예시 : 바둑, 오목, 체스와 같이 현재상태가 이전상태에 의해 영향을 받지 않는 경우


Markov Reward Process
-Markov Process 에서 Reward 와 gamma(감쇠비)가 추가된다
-MRP = (S,P,R,r)(S:상태, P:상태전이확률, R:보상, r: 감쇠비)
-감쇠비의 필요성 : 수학적 편리성, 사람의 선호 반영, 미래에 대한 불확실성 반영

*Monte-Carlo 접근법 : 큰수의 법칙-> 여러번 샘플링을 거친후에 그의 확률을 확인


Markov Decision Process
-MDP는 MRP에서 에이전트가 더해진 것이다
-MDP = (S,A,P,R,r)
-MDP의 전이확률분포(P)는 MRP와는 약간다르다. MDP에서 P는 t시점, s상태에서 에이전트가 a를 하였을 때 s'에 도달할 확률을 의미한다(바둑에서 내가 A를 둔다고 상대방이 항상 같은것을 두는것은 아니기 때문에), Reward(R) 또한 어떤 액션을 했을 때 얻는 보상을 의미한다


정책함수(policy function/pi)
-MDP속 상태에서 s0에서 선택할 수 있는 액션에 대해서 얼만큼의 확률을 부여할 지 정책함수(pi)가 결정함
-정책함수는 에이전트에 속에 있으며 더 큰보상을 얻기 위해 정책을 교정해 나가는 것이 강화학습->CNN의 feature function